{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inicialización de MediaPipe Pose.\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=2, enable_segmentation=True, min_detection_confidence=0.5)\n",
    "\n",
    "# Directorio que contiene los videos.\n",
    "video_directory = 'D:/Codes/EFBA/DataEfba/videos'\n",
    "video_files = [os.path.join(video_directory, f) for f in os.listdir(video_directory) if os.path.isfile(os.path.join(video_directory, f))]\n",
    "excel_directory = 'D:/Codes/EFBA/DataEfba/keypoints'\n",
    "\n",
    "# Procesar cada video en el directorio.\n",
    "for video_file in video_files:\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    keypoints_list = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Conversión del color del frame de BGR a RGB.\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Procesamiento del frame para detectar la pose.\n",
    "        results = pose.process(frame_rgb)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            # Extracción de las coordenadas de los puntos clave.\n",
    "            keypoints = []\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                keypoints.append((landmark.x, landmark.y, landmark.z, landmark.visibility))\n",
    "            keypoints_list.append(keypoints)\n",
    "\n",
    "    cap.release()\n",
    "    \n",
    "    # Opcional: Convertir la lista de puntos clave a un DataFrame de pandas para un análisis más fácil.\n",
    "    df_keypoints = pd.DataFrame(keypoints_list, columns=[f'x{i}' for i in range(11)] + [f'y{i}' for i in range(11)] + [f'z{i}' for i in range(11)])\n",
    "    \n",
    "    # Aquí puedes guardar o procesar df_keypoints como prefieras.\n",
    "    # Por ejemplo, guardar en un archivo CSV.\n",
    "    output_filename = os.path.splitext(os.path.basename(video_file))[0] + '_keypoints.csv'\n",
    "    df_keypoints.to_csv(os.path.join(excel_directory, output_filename), index=False)\n",
    "\n",
    "    print(f\"Procesado {video_file}, resultados guardados en {output_filename}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_column(data):\n",
    "    try:\n",
    "        # Usa ast.literal_eval para convertir la cadena a una tupla\n",
    "        return pd.DataFrame([ast.literal_eval(x) for x in data])\n",
    "    except ValueError:\n",
    "        # En caso de error, regresa un DataFrame vacío\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Preparar un diccionario para recopilar las nuevas columnas\n",
    "    new_data = {}\n",
    "    \n",
    "    # Iterar sobre las columnas originales y procesar cada una\n",
    "    for column in df.columns:\n",
    "        # Si la columna es de tipo object (cadena), procesar su contenido\n",
    "        if df[column].dtype == object:\n",
    "            processed_data = process_column(df[column])\n",
    "            # Si el procesamiento fue exitoso y el DataFrame no está vacío\n",
    "            if not processed_data.empty:\n",
    "                # Crear nuevas columnas para 'x', 'y', 'z' y 'visibility'\n",
    "                new_data[f'{column}_x'] = processed_data[0]\n",
    "                new_data[f'{column}_y'] = processed_data[1]\n",
    "                new_data[f'{column}_z'] = processed_data[2]\n",
    "                new_data[f'{column}_visibility'] = processed_data[3]\n",
    "    \n",
    "    # Crear un nuevo DataFrame con las nuevas columnas\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "    \n",
    "    # Guardar el nuevo DataFrame en un archivo CSV\n",
    "    output_file_path = os.path.join(output_directory, os.path.basename(file_path).replace('.csv', '_processed.csv'))\n",
    "    new_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Archivo procesado y guardado: {output_file_path}\")\n",
    "\n",
    "# Define el directorio donde se encuentran tus archivos CSV\n",
    "directory_path = 'D:/Codes/EFBA/DataEfba/keypoints'\n",
    "output_directory = 'D:/Codes/EFBA/DataEfba/pkeypoints'\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Lista todos los archivos CSV en el directorio\n",
    "csv_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "# Procesa cada archivo CSV en el directorio\n",
    "for file in csv_files:\n",
    "    process_csv(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_features(file_path, output_directory):\n",
    "    # Cargar el DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Define las columnas de características que deseas normalizar\n",
    "    feature_columns = [\n",
    "        'y0_x', 'y0_y', 'y0_z', 'y0_visibility',\n",
    "'y1_x', 'y1_y', 'y1_z', 'y1_visibility',\n",
    "'y2_x', 'y2_y', 'y2_z', 'y2_visibility',\n",
    "'y3_x', 'y3_y', 'y3_z', 'y3_visibility',\n",
    "'y4_x', 'y4_y', 'y4_z', 'y4_visibility',\n",
    "'y5_x', 'y5_y', 'y5_z', 'y5_visibility',\n",
    "'y6_x', 'y6_y', 'y6_z', 'y6_visibility',\n",
    "'y7_x', 'y7_y', 'y7_z', 'y7_visibility',\n",
    "'y8_x', 'y8_y', 'y8_z', 'y8_visibility',\n",
    "'y9_x', 'y9_y', 'y9_z', 'y9_visibility',\n",
    "'y10_x', 'y10_y', 'y10_z', 'y10_visibility',\n",
    "'z0_x', 'z0_y', 'z0_z', 'z0_visibility',\n",
    "'z1_x', 'z1_y', 'z1_z', 'z1_visibility',\n",
    "'z2_x', 'z2_y', 'z2_z', 'z2_visibility',\n",
    "'z3_x', 'z3_y', 'z3_z', 'z3_visibility',\n",
    "'z4_x', 'z4_y', 'z4_z', 'z4_visibility',\n",
    "'z5_x', 'z5_y', 'z5_z', 'z5_visibility',\n",
    "'z6_x', 'z6_y', 'z6_z', 'z6_visibility',\n",
    "'z7_x', 'z7_y', 'z7_z', 'z7_visibility',\n",
    "'z8_x', 'z8_y', 'z8_z', 'z8_visibility',\n",
    "'z9_x', 'z9_y', 'z9_z', 'z9_visibility',\n",
    "'z10_x', 'z10_y', 'z10_z', 'z10_visibility'\n",
    "        # Asegúrate de incluir todas tus características aquí\n",
    "    ]\n",
    "    \n",
    "    # Instancia el objeto StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Aplica la normalización\n",
    "    df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "    \n",
    "    # Construir el path de salida\n",
    "    output_file_path = os.path.join(output_directory, os.path.basename(file_path).replace('.csv', '_normalized.csv'))\n",
    "    \n",
    "    # Guardar el DataFrame normalizado\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Archivo normalizado guardado en: {output_file_path}\")\n",
    "\n",
    "# Define la carpeta donde se encuentran tus archivos CSV y la carpeta de salida\n",
    "input_folder_path = 'D:/Codes/EFBA/DataEfba/pkeypoints/'\n",
    "output_folder_path = 'D:/Codes/EFBA/DataEfba/nkeypoints/'\n",
    "\n",
    "# Crea el directorio de salida si no existe\n",
    "if not os.path.exists(output_folder_path):\n",
    "    os.makedirs(output_folder_path)\n",
    "\n",
    "# Obtener una lista de todos los archivos CSV en la carpeta de entrada\n",
    "csv_files = [os.path.join(input_folder_path, f) for f in os.listdir(input_folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Normalizar las características de cada archivo CSV\n",
    "for file in csv_files:\n",
    "    normalize_features(file, output_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ruta al directorio con los archivos normalizados\n",
    "normalized_folder_path = 'D:/Codes/EFBA/DataEfba/nkeypoints/'\n",
    "\n",
    "# Lista todos los archivos en el directorio\n",
    "files = [os.path.join(normalized_folder_path, f) for f in os.listdir(normalized_folder_path) if f.endswith('_normalized.csv')]\n",
    "\n",
    "# Carga todos los archivos en un DataFrame\n",
    "df_features = pd.concat([pd.read_csv(file) for file in files], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir la ruta al directorio con los archivos normalizados\n",
    "input_directory = 'D:/Codes/EFBA/DataEfba/nkeypoints/'\n",
    "output_directory = 'D:/Codes/EFBA/DataEfba/nkeypoints_wframe'\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Definir la tasa de frames\n",
    "frame_rate = 30  # reemplaza con la tasa de frames real de tus videos\n",
    "\n",
    "# Lista todos los archivos CSV en el directorio de entrada\n",
    "files = [f for f in os.listdir(input_directory) if f.endswith('.csv')]\n",
    "\n",
    "# Procesar cada archivo\n",
    "for file in files:\n",
    "    # Cargar el archivo CSV\n",
    "    df = pd.read_csv(os.path.join(input_directory, file))\n",
    "    \n",
    "    # Añadir la columna de frame asumiendo que la primera fila corresponde al frame 0\n",
    "    df['frame'] = df.index / frame_rate\n",
    "    \n",
    "    # Guardar el DataFrame actualizado en el directorio de salida\n",
    "    df.to_csv(os.path.join(output_directory, file), index=False)\n",
    "\n",
    "print(\"Proceso completado. Los archivos actualizados están en:\", output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Asumiendo que la ruta al archivo es correcta\n",
    "df_annotations = pd.read_csv('D:/Codes/EFBA/DataEfba/Efba_criterios_temp.csv')\n",
    "\n",
    "# Verificar las columnas\n",
    "print(\"Columnas en df_annotations:\", df_annotations.columns)\n",
    "\n",
    "# Si el nombre de la columna está mal escrito o no como lo esperabas, aquí lo verás\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Caminos a las carpetas\n",
    "features_directory = 'D:/Codes/EFBA/DataEfba/nkeypoints_wframe'\n",
    "annotations_path = 'D:/Codes/EFBA/DataEfba/Efba_criterios_temp.csv'\n",
    "output_directory = 'D:/Codes/EFBA/DataEfba'\n",
    "\n",
    "# Cargar las anotaciones\n",
    "df_annotations = pd.read_csv(annotations_path)\n",
    "\n",
    "# Expandir las anotaciones a cada frame relevante\n",
    "expanded_annotations = []\n",
    "for _, row in df_annotations.iterrows():\n",
    "    for frame in range(row['frame_inicio'], row['frame_final'] + 1):\n",
    "        expanded_annotations.append({\n",
    "            'nombre_del_video': row['nombre_del_video'],\n",
    "            'frame': frame,\n",
    "            'criterio': row['criterio'],\n",
    "            'correcto_o_incorrecto': row['correcto_o_incorrecto'],\n",
    "            'jugador': row['jugador']\n",
    "        })\n",
    "\n",
    "# Convertir la lista expandida de anotaciones a un DataFrame\n",
    "df_expanded_annotations = pd.DataFrame(expanded_annotations)\n",
    "\n",
    "# Procesar cada archivo de características\n",
    "for filename in os.listdir(features_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        df_features = pd.read_csv(os.path.join(features_directory, filename))\n",
    "        \n",
    "        # Fusionar los DataFrames basado en 'nombre_del_video' (ajustar si el nombre del archivo es el ID) y 'frame'\n",
    "        df_merged = pd.merge(df_features, df_expanded_annotations, on=['nombre_del_video', 'frame'], how='left')\n",
    "        \n",
    "        # Guardar el DataFrame combinado\n",
    "        df_merged.to_csv(os.path.join(output_directory, f\"{filename[:-4]}_merged.csv\"), index=False)\n",
    "\n",
    "print(\"Todos los archivos han sido procesados y fusionados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al directorio con los archivos fusionados\n",
    "merged_directory = 'D:/Codes/EFBA/DataEfba/mergeddata'\n",
    "\n",
    "# Directorios para guardar los datos escalados\n",
    "scaled_output_directory = 'D:/Codes/EFBA/DataEfba/scaleddata'\n",
    "if not os.path.exists(scaled_output_directory):\n",
    "    os.makedirs(scaled_output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mache12\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1050: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\mache12\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1055: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\mache12\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1075: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesado y guardado: D:/Codes/EFBA/DataEfba/scaleddata\\allen_keypoints_processed_normalized_merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mache12\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1050: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\mache12\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1055: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\mache12\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1075: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesado y guardado: D:/Codes/EFBA/DataEfba/scaleddata\\bird_keypoints_processed_normalized_merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mache12\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1050: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\mache12\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1055: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\mache12\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1075: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesado y guardado: D:/Codes/EFBA/DataEfba/scaleddata\\curry1_keypoints_processed_normalized_merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mache12\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1050: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "C:\\Users\\mache12\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1055: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "C:\\Users\\mache12\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\extmath.py:1075: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesado y guardado: D:/Codes/EFBA/DataEfba/scaleddata\\kerr_keypoints_processed_normalized_merged.csv\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(merged_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(merged_directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Identificar columnas numéricas para escalar\n",
    "        cols_to_scale = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        cols_non_numeric = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "        \n",
    "        # Escalar solo las columnas numéricas\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled = pd.DataFrame(scaler.fit_transform(df[cols_to_scale]), columns=cols_to_scale, index=df.index)\n",
    "        \n",
    "        # Reintegra las columnas no numéricas\n",
    "        df_final_scaled = pd.concat([df_scaled, df[cols_non_numeric]], axis=1)\n",
    "        \n",
    "        # Guardar el DataFrame escalado\n",
    "        output_path = os.path.join(scaled_output_directory, filename)\n",
    "        df_final_scaled.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"Procesado y guardado: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Asumiendo que 'df_final_scaled' es tu DataFrame final que incluye tanto características como etiquetas\n",
    "# Define tus características y etiquetas\n",
    "X = df_final_scaled.drop(['criterio', 'correcto_o_incorrecto'], axis=1)  # Ajusta esto basado en tus nombres de columnas de etiquetas\n",
    "y = df_final_scaled[['criterio', 'correcto_o_incorrecto']]  # Ajusta esto según tus necesidades\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# Inicializar el clasificador RandomForest\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Hacer que el modelo soporte la clasificación multietiqueta\n",
    "model = MultiOutputClassifier(forest, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\multioutput.py:538\u001b[0m, in \u001b[0;36mMultiOutputClassifier.fit\u001b[1;34m(self, X, Y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[0;32m    513\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and targets Y.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \n\u001b[0;32m    515\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;124;03m        Returns a fitted instance.\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m [estimator\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;28;01mfor\u001b[39;00m estimator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_]\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\multioutput.py:242\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe base estimator should implement a fit method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 242\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno_validation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    245\u001b[0m     check_classification_targets(y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:607\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    605\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m--> 607\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validate_separately:\n\u001b[0;32m    610\u001b[0m         \u001b[38;5;66;03m# We need this because some estimators validate X and y\u001b[39;00m\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;66;03m# separately, and in general, separately calling check_array()\u001b[39;00m\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;66;03m# on X and y isn't equivalent to just calling check_X_y()\u001b[39;00m\n\u001b[0;32m    613\u001b[0m         \u001b[38;5;66;03m# :(\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1172\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_output:\n\u001b[1;32m-> 1172\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1182\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    954\u001b[0m         )\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
