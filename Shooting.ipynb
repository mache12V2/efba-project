{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesado D:/Codes/EFBA/DataEfba/videos\\allen.mp4, resultados guardados en allen_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\bird.mp4, resultados guardados en bird_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\curry1.mp4, resultados guardados en curry1_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\curry2.mp4, resultados guardados en curry2_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\harden.mp4, resultados guardados en harden_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\jj.mp4, resultados guardados en jj_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\jordan.mp4, resultados guardados en jordan_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\kd.mp4, resultados guardados en kd_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\kerr.mp4, resultados guardados en kerr_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\klay.mp4, resultados guardados en klay_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\korver.mp4, resultados guardados en korver_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\Lillard.mp4, resultados guardados en Lillard_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\miller.mp4, resultados guardados en miller_keypoints.csv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inicialización de MediaPipe Pose.\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=2, enable_segmentation=True, min_detection_confidence=0.5)\n",
    "\n",
    "# Directorio que contiene los videos.\n",
    "video_directory = 'D:/Codes/EFBA/DataEfba/videos'\n",
    "video_files = [os.path.join(video_directory, f) for f in os.listdir(video_directory) if os.path.isfile(os.path.join(video_directory, f))]\n",
    "excel_directory = 'D:/Codes/EFBA/DataEfba/keypoints'\n",
    "\n",
    "# Procesar cada video en el directorio.\n",
    "for video_file in video_files:\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    keypoints_list = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Conversión del color del frame de BGR a RGB.\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Procesamiento del frame para detectar la pose.\n",
    "        results = pose.process(frame_rgb)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            # Extracción de las coordenadas de los puntos clave.\n",
    "            keypoints = []\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                keypoints.append((landmark.x, landmark.y, landmark.z, landmark.visibility))\n",
    "            keypoints_list.append(keypoints)\n",
    "\n",
    "    cap.release()\n",
    "    \n",
    "    # Opcional: Convertir la lista de puntos clave a un DataFrame de pandas para un análisis más fácil.\n",
    "    df_keypoints = pd.DataFrame(keypoints_list, columns=[f'x{i}' for i in range(11)] + [f'y{i}' for i in range(11)] + [f'z{i}' for i in range(11)])\n",
    "    \n",
    "    # Aquí puedes guardar o procesar df_keypoints como prefieras.\n",
    "    # Por ejemplo, guardar en un archivo CSV.\n",
    "    output_filename = os.path.splitext(os.path.basename(video_file))[0] + '_keypoints.csv'\n",
    "    df_keypoints.to_csv(os.path.join(excel_directory, output_filename), index=False)\n",
    "\n",
    "    print(f\"Procesado {video_file}, resultados guardados en {output_filename}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\allen_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\bird_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\curry1_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\curry2_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\harden_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\jj_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\jordan_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\kd_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\kerr_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\klay_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\korver_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\Lillard_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\miller_keypoints_processed.csv\n"
     ]
    }
   ],
   "source": [
    "def process_column(data):\n",
    "    try:\n",
    "        # Usa ast.literal_eval para convertir la cadena a una tupla\n",
    "        return pd.DataFrame([ast.literal_eval(x) for x in data])\n",
    "    except ValueError:\n",
    "        # En caso de error, regresa un DataFrame vacío\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Preparar un diccionario para recopilar las nuevas columnas\n",
    "    new_data = {}\n",
    "    \n",
    "    # Iterar sobre las columnas originales y procesar cada una\n",
    "    for column in df.columns:\n",
    "        # Si la columna es de tipo object (cadena), procesar su contenido\n",
    "        if df[column].dtype == object:\n",
    "            processed_data = process_column(df[column])\n",
    "            # Si el procesamiento fue exitoso y el DataFrame no está vacío\n",
    "            if not processed_data.empty:\n",
    "                # Crear nuevas columnas para 'x', 'y', 'z' y 'visibility'\n",
    "                new_data[f'{column}_x'] = processed_data[0]\n",
    "                new_data[f'{column}_y'] = processed_data[1]\n",
    "                new_data[f'{column}_z'] = processed_data[2]\n",
    "                new_data[f'{column}_visibility'] = processed_data[3]\n",
    "    \n",
    "    # Crear un nuevo DataFrame con las nuevas columnas\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "    \n",
    "    # Guardar el nuevo DataFrame en un archivo CSV\n",
    "    output_file_path = os.path.join(output_directory, os.path.basename(file_path).replace('.csv', '_processed.csv'))\n",
    "    new_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Archivo procesado y guardado: {output_file_path}\")\n",
    "\n",
    "# Define el directorio donde se encuentran tus archivos CSV\n",
    "directory_path = 'D:/Codes/EFBA/DataEfba/keypoints'\n",
    "output_directory = 'D:/Codes/EFBA/DataEfba/pkeypoints'\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Lista todos los archivos CSV en el directorio\n",
    "csv_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "# Procesa cada archivo CSV en el directorio\n",
    "for file in csv_files:\n",
    "    process_csv(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archvio normalizado guardado en: D:/Codes/EFBA/DataEfba/pkeypoints\\allen_keypoints_processed_normalized.csv\n",
      "Archvio normalizado guardado en: D:/Codes/EFBA/DataEfba/pkeypoints\\bird_keypoints_processed_normalized.csv\n",
      "Archvio normalizado guardado en: D:/Codes/EFBA/DataEfba/pkeypoints\\curry1_keypoints_processed_normalized.csv\n",
      "Archvio normalizado guardado en: D:/Codes/EFBA/DataEfba/pkeypoints\\curry2_keypoints_processed_normalized.csv\n",
      "Archvio normalizado guardado en: D:/Codes/EFBA/DataEfba/pkeypoints\\harden_keypoints_processed_normalized.csv\n",
      "Archvio normalizado guardado en: D:/Codes/EFBA/DataEfba/pkeypoints\\jj_keypoints_processed_normalized.csv\n",
      "Archvio normalizado guardado en: D:/Codes/EFBA/DataEfba/pkeypoints\\jordan_keypoints_processed_normalized.csv\n",
      "Archvio normalizado guardado en: D:/Codes/EFBA/DataEfba/pkeypoints\\kd_keypoints_processed_normalized.csv\n",
      "Archvio normalizado guardado en: D:/Codes/EFBA/DataEfba/pkeypoints\\kerr_keypoints_processed_normalized.csv\n",
      "Archvio normalizado guardado en: D:/Codes/EFBA/DataEfba/pkeypoints\\klay_keypoints_processed_normalized.csv\n",
      "Archvio normalizado guardado en: D:/Codes/EFBA/DataEfba/pkeypoints\\korver_keypoints_processed_normalized.csv\n",
      "Archvio normalizado guardado en: D:/Codes/EFBA/DataEfba/pkeypoints\\Lillard_keypoints_processed_normalized.csv\n",
      "Archvio normalizado guardado en: D:/Codes/EFBA/DataEfba/pkeypoints\\miller_keypoints_processed_normalized.csv\n"
     ]
    }
   ],
   "source": [
    "def normalize_features(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    feature_columns = ['y0_x', 'y0_y', 'y0_z', 'y0_visibility',\n",
    "'y1_x', 'y1_y', 'y1_z', 'y1_visibility',\n",
    "'y2_x', 'y2_y', 'y2_z', 'y2_visibility',\n",
    "'y3_x', 'y3_y', 'y3_z', 'y3_visibility',\n",
    "'y4_x', 'y4_y', 'y4_z', 'y4_visibility',\n",
    "'y5_x', 'y5_y', 'y5_z', 'y5_visibility',\n",
    "'y6_x', 'y6_y', 'y6_z', 'y6_visibility',\n",
    "'y7_x', 'y7_y', 'y7_z', 'y7_visibility',\n",
    "'y8_x', 'y8_y', 'y8_z', 'y8_visibility',\n",
    "'y9_x', 'y9_y', 'y9_z', 'y9_visibility',\n",
    "'y10_x', 'y10_y', 'y10_z', 'y10_visibility',\n",
    "'z0_x', 'z0_y', 'z0_z', 'z0_visibility',\n",
    "'z1_x', 'z1_y', 'z1_z', 'z1_visibility',\n",
    "'z2_x', 'z2_y', 'z2_z', 'z2_visibility',\n",
    "'z3_x', 'z3_y', 'z3_z', 'z3_visibility',\n",
    "'z4_x', 'z4_y', 'z4_z', 'z4_visibility',\n",
    "'z5_x', 'z5_y', 'z5_z', 'z5_visibility',\n",
    "'z6_x', 'z6_y', 'z6_z', 'z6_visibility',\n",
    "'z7_x', 'z7_y', 'z7_z', 'z7_visibility',\n",
    "'z8_x', 'z8_y', 'z8_z', 'z8_visibility',\n",
    "'z9_x', 'z9_y', 'z9_z', 'z9_visibility',\n",
    "'z10_x', 'z10_y', 'z10_z', 'z10_visibility']\n",
    "    features = df[feature_columns]\n",
    "    features_normalized = (features -  features.mean())/features.std()\n",
    "    df[feature_columns] =  features_normalized\n",
    "    \n",
    "    normalize_file_path =  file_path.replace('.csv','_normalized.csv')\n",
    "    df.to_csv(normalize_file_path,index= False)\n",
    "    print(f\"Archvio normalizado guardado en: {normalize_file_path}\")\n",
    "\n",
    "folder_path = 'D:/Codes/EFBA/DataEfba/pkeypoints'\n",
    "\n",
    "csv_files = [os.path.join(folder_path,f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "for file in csv_files:\n",
    "    normalize_features(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ''",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Suponiendo que 'features_normalized' son tus características y tienes una columna 'target' en tu DataFrame que representa la variable objetivo.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m X \u001b[38;5;241m=\u001b[39m features_normalized  \u001b[38;5;66;03m# Características\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Variable objetivo\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Dividir los datos en conjunto de entrenamiento y prueba\u001b[39;00m\n\u001b[0;32m      6\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "\n",
    "# Suponiendo que 'features_normalized' son tus características y tienes una columna 'target' en tu DataFrame que representa la variable objetivo.\n",
    "X = features_normalized  # Características\n",
    "y = df['']  # Variable objetivo\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Si también quieres un conjunto de validación, puedes dividir el conjunto de entrenamiento una vez más\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "# Ahora tienes:\n",
    "# - 60% de los datos en el conjunto de entrenamiento\n",
    "# - 20% de los datos en el conjunto de validación\n",
    "# - 20% de los datos en el conjunto de prueba\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
