{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesado D:/Codes/EFBA/DataEfba/videos\\allen.mp4, resultados guardados en allen_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\bird.mp4, resultados guardados en bird_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\curry1.mp4, resultados guardados en curry1_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\curry2.mp4, resultados guardados en curry2_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\harden.mp4, resultados guardados en harden_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\jj.mp4, resultados guardados en jj_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\jordan.mp4, resultados guardados en jordan_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\kd.mp4, resultados guardados en kd_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\kerr.mp4, resultados guardados en kerr_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\klay.mp4, resultados guardados en klay_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\korver.mp4, resultados guardados en korver_keypoints.csv.\n",
      "Procesado D:/Codes/EFBA/DataEfba/videos\\Lillard.mp4, resultados guardados en Lillard_keypoints.csv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inicialización de MediaPipe Pose.\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=2, enable_segmentation=True, min_detection_confidence=0.5)\n",
    "\n",
    "# Directorio que contiene los videos.\n",
    "video_directory = 'D:/Codes/EFBA/DataEfba/videos'\n",
    "video_files = [os.path.join(video_directory, f) for f in os.listdir(video_directory) if os.path.isfile(os.path.join(video_directory, f))]\n",
    "excel_directory = 'D:/Codes/EFBA/DataEfba/keypoints'\n",
    "\n",
    "# Procesar cada video en el directorio.\n",
    "for video_file in video_files:\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    keypoints_list = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Conversión del color del frame de BGR a RGB.\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Procesamiento del frame para detectar la pose.\n",
    "        results = pose.process(frame_rgb)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            # Extracción de las coordenadas de los puntos clave.\n",
    "            keypoints = []\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                keypoints.append((landmark.x, landmark.y, landmark.z, landmark.visibility))\n",
    "            keypoints_list.append(keypoints)\n",
    "\n",
    "    cap.release()\n",
    "    \n",
    "    # Opcional: Convertir la lista de puntos clave a un DataFrame de pandas para un análisis más fácil.\n",
    "    df_keypoints = pd.DataFrame(keypoints_list, columns=[f'x{i}' for i in range(11)] + [f'y{i}' for i in range(11)] + [f'z{i}' for i in range(11)])\n",
    "    \n",
    "    # Aquí puedes guardar o procesar df_keypoints como prefieras.\n",
    "    # Por ejemplo, guardar en un archivo CSV.\n",
    "    output_filename = os.path.splitext(os.path.basename(video_file))[0] + '_keypoints.csv'\n",
    "    df_keypoints.to_csv(os.path.join(excel_directory, output_filename), index=False)\n",
    "\n",
    "    print(f\"Procesado {video_file}, resultados guardados en {output_filename}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\allen_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\bird_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\curry1_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\curry2_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\harden_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\jj_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\jordan_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\kd_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\kerr_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\klay_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\korver_keypoints_processed.csv\n",
      "Archivo procesado y guardado: D:/Codes/EFBA/DataEfba/pkeypoints\\Lillard_keypoints_processed.csv\n"
     ]
    }
   ],
   "source": [
    "def process_column(data):\n",
    "    try:\n",
    "        # Usa ast.literal_eval para convertir la cadena a una tupla\n",
    "        return pd.DataFrame([ast.literal_eval(x) for x in data])\n",
    "    except ValueError:\n",
    "        # En caso de error, regresa un DataFrame vacío\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Preparar un diccionario para recopilar las nuevas columnas\n",
    "    new_data = {}\n",
    "    \n",
    "    # Iterar sobre las columnas originales y procesar cada una\n",
    "    for column in df.columns:\n",
    "        # Si la columna es de tipo object (cadena), procesar su contenido\n",
    "        if df[column].dtype == object:\n",
    "            processed_data = process_column(df[column])\n",
    "            # Si el procesamiento fue exitoso y el DataFrame no está vacío\n",
    "            if not processed_data.empty:\n",
    "                # Crear nuevas columnas para 'x', 'y', 'z' y 'visibility'\n",
    "                new_data[f'{column}_x'] = processed_data[0]\n",
    "                new_data[f'{column}_y'] = processed_data[1]\n",
    "                new_data[f'{column}_z'] = processed_data[2]\n",
    "                new_data[f'{column}_visibility'] = processed_data[3]\n",
    "    \n",
    "    # Crear un nuevo DataFrame con las nuevas columnas\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "    \n",
    "    # Guardar el nuevo DataFrame en un archivo CSV\n",
    "    output_file_path = os.path.join(output_directory, os.path.basename(file_path).replace('.csv', '_processed.csv'))\n",
    "    new_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Archivo procesado y guardado: {output_file_path}\")\n",
    "\n",
    "# Define el directorio donde se encuentran tus archivos CSV\n",
    "directory_path = 'D:/Codes/EFBA/DataEfba/keypoints'\n",
    "output_directory = 'D:/Codes/EFBA/DataEfba/pkeypoints'\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Lista todos los archivos CSV en el directorio\n",
    "csv_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "# Procesa cada archivo CSV en el directorio\n",
    "for file in csv_files:\n",
    "    process_csv(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo normalizado guardado en: D:/Codes/EFBA/DataEfba/nkeypoints/allen_keypoints_processed_normalized.csv\n",
      "Archivo normalizado guardado en: D:/Codes/EFBA/DataEfba/nkeypoints/bird_keypoints_processed_normalized.csv\n",
      "Archivo normalizado guardado en: D:/Codes/EFBA/DataEfba/nkeypoints/curry1_keypoints_processed_normalized.csv\n",
      "Archivo normalizado guardado en: D:/Codes/EFBA/DataEfba/nkeypoints/curry2_keypoints_processed_normalized.csv\n",
      "Archivo normalizado guardado en: D:/Codes/EFBA/DataEfba/nkeypoints/harden_keypoints_processed_normalized.csv\n",
      "Archivo normalizado guardado en: D:/Codes/EFBA/DataEfba/nkeypoints/jj_keypoints_processed_normalized.csv\n",
      "Archivo normalizado guardado en: D:/Codes/EFBA/DataEfba/nkeypoints/jordan_keypoints_processed_normalized.csv\n",
      "Archivo normalizado guardado en: D:/Codes/EFBA/DataEfba/nkeypoints/kd_keypoints_processed_normalized.csv\n",
      "Archivo normalizado guardado en: D:/Codes/EFBA/DataEfba/nkeypoints/kerr_keypoints_processed_normalized.csv\n",
      "Archivo normalizado guardado en: D:/Codes/EFBA/DataEfba/nkeypoints/klay_keypoints_processed_normalized.csv\n",
      "Archivo normalizado guardado en: D:/Codes/EFBA/DataEfba/nkeypoints/korver_keypoints_processed_normalized.csv\n",
      "Archivo normalizado guardado en: D:/Codes/EFBA/DataEfba/nkeypoints/Lillard_keypoints_processed_normalized.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_features(file_path, output_directory):\n",
    "    # Cargar el DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Define las columnas de características que deseas normalizar\n",
    "    feature_columns = [\n",
    "        'y0_x', 'y0_y', 'y0_z', 'y0_visibility',\n",
    "'y1_x', 'y1_y', 'y1_z', 'y1_visibility',\n",
    "'y2_x', 'y2_y', 'y2_z', 'y2_visibility',\n",
    "'y3_x', 'y3_y', 'y3_z', 'y3_visibility',\n",
    "'y4_x', 'y4_y', 'y4_z', 'y4_visibility',\n",
    "'y5_x', 'y5_y', 'y5_z', 'y5_visibility',\n",
    "'y6_x', 'y6_y', 'y6_z', 'y6_visibility',\n",
    "'y7_x', 'y7_y', 'y7_z', 'y7_visibility',\n",
    "'y8_x', 'y8_y', 'y8_z', 'y8_visibility',\n",
    "'y9_x', 'y9_y', 'y9_z', 'y9_visibility',\n",
    "'y10_x', 'y10_y', 'y10_z', 'y10_visibility',\n",
    "'z0_x', 'z0_y', 'z0_z', 'z0_visibility',\n",
    "'z1_x', 'z1_y', 'z1_z', 'z1_visibility',\n",
    "'z2_x', 'z2_y', 'z2_z', 'z2_visibility',\n",
    "'z3_x', 'z3_y', 'z3_z', 'z3_visibility',\n",
    "'z4_x', 'z4_y', 'z4_z', 'z4_visibility',\n",
    "'z5_x', 'z5_y', 'z5_z', 'z5_visibility',\n",
    "'z6_x', 'z6_y', 'z6_z', 'z6_visibility',\n",
    "'z7_x', 'z7_y', 'z7_z', 'z7_visibility',\n",
    "'z8_x', 'z8_y', 'z8_z', 'z8_visibility',\n",
    "'z9_x', 'z9_y', 'z9_z', 'z9_visibility',\n",
    "'z10_x', 'z10_y', 'z10_z', 'z10_visibility'\n",
    "        # Asegúrate de incluir todas tus características aquí\n",
    "    ]\n",
    "    \n",
    "    # Instancia el objeto StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Aplica la normalización\n",
    "    df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "    \n",
    "    # Construir el path de salida\n",
    "    output_file_path = os.path.join(output_directory, os.path.basename(file_path).replace('.csv', '_normalized.csv'))\n",
    "    \n",
    "    # Guardar el DataFrame normalizado\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Archivo normalizado guardado en: {output_file_path}\")\n",
    "\n",
    "# Define la carpeta donde se encuentran tus archivos CSV y la carpeta de salida\n",
    "input_folder_path = 'D:/Codes/EFBA/DataEfba/pkeypoints/'\n",
    "output_folder_path = 'D:/Codes/EFBA/DataEfba/nkeypoints/'\n",
    "\n",
    "# Crea el directorio de salida si no existe\n",
    "if not os.path.exists(output_folder_path):\n",
    "    os.makedirs(output_folder_path)\n",
    "\n",
    "# Obtener una lista de todos los archivos CSV en la carpeta de entrada\n",
    "csv_files = [os.path.join(input_folder_path, f) for f in os.listdir(input_folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Normalizar las características de cada archivo CSV\n",
    "for file in csv_files:\n",
    "    normalize_features(file, output_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ruta al directorio con los archivos normalizados\n",
    "normalized_folder_path = 'D:/Codes/EFBA/DataEfba/nkeypoints/'\n",
    "\n",
    "# Lista todos los archivos en el directorio\n",
    "files = [os.path.join(normalized_folder_path, f) for f in os.listdir(normalized_folder_path) if f.endswith('_normalized.csv')]\n",
    "\n",
    "# Carga todos los archivos en un DataFrame\n",
    "df_features = pd.concat([pd.read_csv(file) for file in files], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado. Los archivos actualizados están en: D:/Codes/EFBA/DataEfba/nkeypoints_wframe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Definir la ruta al directorio con los archivos normalizados\n",
    "input_directory = 'D:/Codes/EFBA/DataEfba/nkeypoints/'\n",
    "output_directory = 'D:/Codes/EFBA/DataEfba/nkeypoints_wframe'\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Definir la tasa de frames\n",
    "frame_rate = 30  # reemplaza con la tasa de frames real de tus videos\n",
    "\n",
    "# Lista todos los archivos CSV en el directorio de entrada\n",
    "files = [f for f in os.listdir(input_directory) if f.endswith('.csv')]\n",
    "\n",
    "# Procesar cada archivo\n",
    "for file in files:\n",
    "    # Cargar el archivo CSV\n",
    "    df = pd.read_csv(os.path.join(input_directory, file))\n",
    "    \n",
    "    # Añadir la columna de frame asumiendo que la primera fila corresponde al frame 0\n",
    "    df['frame'] = df.index / frame_rate\n",
    "    \n",
    "    # Guardar el DataFrame actualizado en el directorio de salida\n",
    "    df.to_csv(os.path.join(output_directory, file), index=False)\n",
    "\n",
    "print(\"Proceso completado. Los archivos actualizados están en:\", output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas en df_annotations: Index([' frame_inicio', 'frame_final', 'nombre_del_video', 'criterio',\n",
      "       'correcto_o_incorrecto', 'm_tiempo', 'jugador'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Asumiendo que la ruta al archivo es correcta\n",
    "df_annotations = pd.read_csv('D:/Codes/EFBA/DataEfba/Efba_criterios_temp.csv')\n",
    "\n",
    "# Verificar las columnas\n",
    "print(\"Columnas en df_annotations:\", df_annotations.columns)\n",
    "\n",
    "# Si el nombre de la columna está mal escrito o no como lo esperabas, aquí lo verás\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'nombre_del_video'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16076\\3312245772.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mdf_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Fusionar los DataFrames basado en 'nombre_del_video' (ajustar si el nombre del archivo es el ID) y 'frame'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mdf_merged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_expanded_annotations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nombre_del_video'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'frame'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m# Guardar el DataFrame combinado\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mdf_merged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"{filename[:-4]}_merged.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         )\n\u001b[0;32m    168\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    170\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1287\u001b[1;33m                         \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1288\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1289\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m                         \u001b[1;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1840\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1844\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'nombre_del_video'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Caminos a las carpetas\n",
    "features_directory = 'D:/Codes/EFBA/DataEfba/nkeypoints_wframe'\n",
    "annotations_path = 'D:/Codes/EFBA/DataEfba/Efba_criterios_temp.csv'\n",
    "output_directory = 'D:/Codes/EFBA/DataEfba'\n",
    "\n",
    "# Cargar las anotaciones\n",
    "df_annotations = pd.read_csv(annotations_path)\n",
    "\n",
    "# Expandir las anotaciones a cada frame relevante\n",
    "expanded_annotations = []\n",
    "for _, row in df_annotations.iterrows():\n",
    "    for frame in range(row['frame_inicio'], row['frame_final'] + 1):\n",
    "        expanded_annotations.append({\n",
    "            'nombre_del_video': row['nombre_del_video'],\n",
    "            'frame': frame,\n",
    "            'criterio': row['criterio'],\n",
    "            'correcto_o_incorrecto': row['correcto_o_incorrecto'],\n",
    "            'jugador': row['jugador']\n",
    "        })\n",
    "\n",
    "# Convertir la lista expandida de anotaciones a un DataFrame\n",
    "df_expanded_annotations = pd.DataFrame(expanded_annotations)\n",
    "\n",
    "# Procesar cada archivo de características\n",
    "for filename in os.listdir(features_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        df_features = pd.read_csv(os.path.join(features_directory, filename))\n",
    "        \n",
    "        # Fusionar los DataFrames basado en 'nombre_del_video' (ajustar si el nombre del archivo es el ID) y 'frame'\n",
    "        df_merged = pd.merge(df_features, df_expanded_annotations, on=['nombre_del_video', 'frame'], how='left')\n",
    "        \n",
    "        # Guardar el DataFrame combinado\n",
    "        df_merged.to_csv(os.path.join(output_directory, f\"{filename[:-4]}_merged.csv\"), index=False)\n",
    "\n",
    "print(\"Todos los archivos han sido procesados y fusionados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([' frame_inicio', 'frame_final', 'nombre_del_video', 'criterio',\n",
      "       'correcto_o_incorrecto', 'm_tiempo', 'jugador'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_annotations.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo de etiquetas anotadas\n",
    "labels_file_path = 'D:/Codes/EFBA/DataEfba/Efba_criterios_temp.csv'\n",
    "\n",
    "# Cargar las etiquetas\n",
    "df_labels = pd.read_csv(labels_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: El número de filas en las características y las etiquetas no coincide.\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de que ambos DataFrames estén alineados y tengan la misma longitud\n",
    "if len(df_features) == len(df_labels):\n",
    "    df = pd.concat([df_features, df_labels], axis=1)\n",
    "else:\n",
    "    print(\"Error: El número de filas en las características y las etiquetas no coincide.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Definir las columnas de características y etiquetas\n",
    "features_columns = list(df_features.columns)  # Lista de todas tus columnas de características\n",
    "label_columns = list(df_labels.columns)  # Lista de todas tus columnas de etiquetas\n",
    "\n",
    "X = df[features_columns]  # Características\n",
    "y = df[label_columns]  # Etiquetas\n",
    "\n",
    "# Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
