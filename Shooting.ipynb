{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inicialización de MediaPipe Pose.\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=2, enable_segmentation=True, min_detection_confidence=0.5)\n",
    "\n",
    "# Directorio que contiene los videos.\n",
    "video_directory = 'D:/Codes/EFBA/DataEfba/videos'\n",
    "video_files = [os.path.join(video_directory, f) for f in os.listdir(video_directory) if os.path.isfile(os.path.join(video_directory, f))]\n",
    "excel_directory = 'D:/Codes/EFBA/DataEfba/keypoints'\n",
    "\n",
    "# Procesar cada video en el directorio.\n",
    "for video_file in video_files:\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    keypoints_list = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Conversión del color del frame de BGR a RGB.\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Procesamiento del frame para detectar la pose.\n",
    "        results = pose.process(frame_rgb)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            # Extracción de las coordenadas de los puntos clave.\n",
    "            keypoints = []\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                keypoints.append((landmark.x, landmark.y, landmark.z, landmark.visibility))\n",
    "            keypoints_list.append(keypoints)\n",
    "\n",
    "    cap.release()\n",
    "    \n",
    "    # Opcional: Convertir la lista de puntos clave a un DataFrame de pandas para un análisis más fácil.\n",
    "    df_keypoints = pd.DataFrame(keypoints_list, columns=[f'x{i}' for i in range(11)] + [f'y{i}' for i in range(11)] + [f'z{i}' for i in range(11)])\n",
    "    \n",
    "    # Aquí puedes guardar o procesar df_keypoints como prefieras.\n",
    "    # Por ejemplo, guardar en un archivo CSV.\n",
    "    output_filename = os.path.splitext(os.path.basename(video_file))[0] + '_keypoints.csv'\n",
    "    df_keypoints.to_csv(os.path.join(excel_directory, output_filename), index=False)\n",
    "\n",
    "    print(f\"Procesado {video_file}, resultados guardados en {output_filename}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_column(data):\n",
    "    try:\n",
    "        # Usa ast.literal_eval para convertir la cadena a una tupla\n",
    "        return pd.DataFrame([ast.literal_eval(x) for x in data])\n",
    "    except ValueError:\n",
    "        # En caso de error, regresa un DataFrame vacío\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Preparar un diccionario para recopilar las nuevas columnas\n",
    "    new_data = {}\n",
    "    \n",
    "    # Iterar sobre las columnas originales y procesar cada una\n",
    "    for column in df.columns:\n",
    "        # Si la columna es de tipo object (cadena), procesar su contenido\n",
    "        if df[column].dtype == object:\n",
    "            processed_data = process_column(df[column])\n",
    "            # Si el procesamiento fue exitoso y el DataFrame no está vacío\n",
    "            if not processed_data.empty:\n",
    "                # Crear nuevas columnas para 'x', 'y', 'z' y 'visibility'\n",
    "                new_data[f'{column}_x'] = processed_data[0]\n",
    "                new_data[f'{column}_y'] = processed_data[1]\n",
    "                new_data[f'{column}_z'] = processed_data[2]\n",
    "                new_data[f'{column}_visibility'] = processed_data[3]\n",
    "    \n",
    "    # Crear un nuevo DataFrame con las nuevas columnas\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "    \n",
    "    # Guardar el nuevo DataFrame en un archivo CSV\n",
    "    output_file_path = os.path.join(output_directory, os.path.basename(file_path).replace('.csv', '_processed.csv'))\n",
    "    new_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Archivo procesado y guardado: {output_file_path}\")\n",
    "\n",
    "# Define el directorio donde se encuentran tus archivos CSV\n",
    "directory_path = 'D:/Codes/EFBA/DataEfba/keypoints'\n",
    "output_directory = 'D:/Codes/EFBA/DataEfba/pkeypoints'\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Lista todos los archivos CSV en el directorio\n",
    "csv_files = [os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "\n",
    "# Procesa cada archivo CSV en el directorio\n",
    "for file in csv_files:\n",
    "    process_csv(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_features(file_path, output_directory):\n",
    "    # Cargar el DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Define las columnas de características que deseas normalizar\n",
    "    feature_columns = [\n",
    "        'y0_x', 'y0_y', 'y0_z', 'y0_visibility',\n",
    "'y1_x', 'y1_y', 'y1_z', 'y1_visibility',\n",
    "'y2_x', 'y2_y', 'y2_z', 'y2_visibility',\n",
    "'y3_x', 'y3_y', 'y3_z', 'y3_visibility',\n",
    "'y4_x', 'y4_y', 'y4_z', 'y4_visibility',\n",
    "'y5_x', 'y5_y', 'y5_z', 'y5_visibility',\n",
    "'y6_x', 'y6_y', 'y6_z', 'y6_visibility',\n",
    "'y7_x', 'y7_y', 'y7_z', 'y7_visibility',\n",
    "'y8_x', 'y8_y', 'y8_z', 'y8_visibility',\n",
    "'y9_x', 'y9_y', 'y9_z', 'y9_visibility',\n",
    "'y10_x', 'y10_y', 'y10_z', 'y10_visibility',\n",
    "'z0_x', 'z0_y', 'z0_z', 'z0_visibility',\n",
    "'z1_x', 'z1_y', 'z1_z', 'z1_visibility',\n",
    "'z2_x', 'z2_y', 'z2_z', 'z2_visibility',\n",
    "'z3_x', 'z3_y', 'z3_z', 'z3_visibility',\n",
    "'z4_x', 'z4_y', 'z4_z', 'z4_visibility',\n",
    "'z5_x', 'z5_y', 'z5_z', 'z5_visibility',\n",
    "'z6_x', 'z6_y', 'z6_z', 'z6_visibility',\n",
    "'z7_x', 'z7_y', 'z7_z', 'z7_visibility',\n",
    "'z8_x', 'z8_y', 'z8_z', 'z8_visibility',\n",
    "'z9_x', 'z9_y', 'z9_z', 'z9_visibility',\n",
    "'z10_x', 'z10_y', 'z10_z', 'z10_visibility'\n",
    "        # Asegúrate de incluir todas tus características aquí\n",
    "    ]\n",
    "    \n",
    "    # Instancia el objeto StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Aplica la normalización\n",
    "    df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "    \n",
    "    # Construir el path de salida\n",
    "    output_file_path = os.path.join(output_directory, os.path.basename(file_path).replace('.csv', '_normalized.csv'))\n",
    "    \n",
    "    # Guardar el DataFrame normalizado\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Archivo normalizado guardado en: {output_file_path}\")\n",
    "\n",
    "# Define la carpeta donde se encuentran tus archivos CSV y la carpeta de salida\n",
    "input_folder_path = 'D:/Codes/EFBA/DataEfba/pkeypoints/'\n",
    "output_folder_path = 'D:/Codes/EFBA/DataEfba/nkeypoints/'\n",
    "\n",
    "# Crea el directorio de salida si no existe\n",
    "if not os.path.exists(output_folder_path):\n",
    "    os.makedirs(output_folder_path)\n",
    "\n",
    "# Obtener una lista de todos los archivos CSV en la carpeta de entrada\n",
    "csv_files = [os.path.join(input_folder_path, f) for f in os.listdir(input_folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# Normalizar las características de cada archivo CSV\n",
    "for file in csv_files:\n",
    "    normalize_features(file, output_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ruta al directorio con los archivos normalizados\n",
    "normalized_folder_path = 'D:/Codes/EFBA/DataEfba/nkeypoints/'\n",
    "\n",
    "# Lista todos los archivos en el directorio\n",
    "files = [os.path.join(normalized_folder_path, f) for f in os.listdir(normalized_folder_path) if f.endswith('_normalized.csv')]\n",
    "\n",
    "# Carga todos los archivos en un DataFrame\n",
    "df_features = pd.concat([pd.read_csv(file) for file in files], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir la ruta al directorio con los archivos normalizados\n",
    "input_directory = 'D:/Codes/EFBA/DataEfba/nkeypoints/'\n",
    "output_directory = 'D:/Codes/EFBA/DataEfba/nkeypoints_wframe'\n",
    "\n",
    "# Crear el directorio de salida si no existe\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Definir la tasa de frames\n",
    "frame_rate = 30  # reemplaza con la tasa de frames real de tus videos\n",
    "\n",
    "# Lista todos los archivos CSV en el directorio de entrada\n",
    "files = [f for f in os.listdir(input_directory) if f.endswith('.csv')]\n",
    "\n",
    "# Procesar cada archivo\n",
    "for file in files:\n",
    "    # Cargar el archivo CSV\n",
    "    df = pd.read_csv(os.path.join(input_directory, file))\n",
    "    \n",
    "    # Añadir la columna de frame asumiendo que la primera fila corresponde al frame 0\n",
    "    df['frame'] = df.index / frame_rate\n",
    "    \n",
    "    # Guardar el DataFrame actualizado en el directorio de salida\n",
    "    df.to_csv(os.path.join(output_directory, file), index=False)\n",
    "\n",
    "print(\"Proceso completado. Los archivos actualizados están en:\", output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Asumiendo que la ruta al archivo es correcta\n",
    "df_annotations = pd.read_csv('D:/Codes/EFBA/DataEfba/Efba_criterios_temp.csv')\n",
    "\n",
    "# Verificar las columnas\n",
    "print(\"Columnas en df_annotations:\", df_annotations.columns)\n",
    "\n",
    "# Si el nombre de la columna está mal escrito o no como lo esperabas, aquí lo verás\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_features.columns)\n",
    "print(df_annotations.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_features.head())\n",
    "print(df_annotations.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mache12\\AppData\\Local\\Temp\\ipykernel_22820\\2692737078.py:30: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation.\n",
      "  df_merged = pd.merge(df_features, df_expanded_annotations, on=['nombre_del_video', 'frame'], how='left')\n",
      "C:\\Users\\mache12\\AppData\\Local\\Temp\\ipykernel_22820\\2692737078.py:30: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation.\n",
      "  df_merged = pd.merge(df_features, df_expanded_annotations, on=['nombre_del_video', 'frame'], how='left')\n",
      "C:\\Users\\mache12\\AppData\\Local\\Temp\\ipykernel_22820\\2692737078.py:30: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation.\n",
      "  df_merged = pd.merge(df_features, df_expanded_annotations, on=['nombre_del_video', 'frame'], how='left')\n",
      "C:\\Users\\mache12\\AppData\\Local\\Temp\\ipykernel_22820\\2692737078.py:30: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation.\n",
      "  df_merged = pd.merge(df_features, df_expanded_annotations, on=['nombre_del_video', 'frame'], how='left')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los archivos han sido procesados y fusionados.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Caminos a las carpetas\n",
    "features_directory = 'D:/Codes/EFBA/DataEfba/nkeypoints_wframe'\n",
    "annotations_path = 'D:/Codes/EFBA/DataEfba/Efba_criterios_temp.csv'\n",
    "output_directory = 'D:/Codes/EFBA/DataEfba/mergeddata'\n",
    "\n",
    "# Cargar las anotaciones\n",
    "df_annotations = pd.read_csv(annotations_path)\n",
    "\n",
    "# Expandir las anotaciones a cada frame relevante\n",
    "expanded_annotations = []\n",
    "for _, row in df_annotations.iterrows():\n",
    "    for frame in range(row['frame_inicio'], row['frame_final'] + 1):\n",
    "        expanded_annotations.append({\n",
    "            'nombre_del_video': row['nombre_del_video'],\n",
    "            'frame': frame,\n",
    "            'criterio': row['criterio'],\n",
    "            'correcto_o_incorrecto': row['correcto_o_incorrecto'],\n",
    "            'jugador': row['jugador']\n",
    "        })\n",
    "\n",
    "# Convertir la lista expandida de anotaciones a un DataFrame\n",
    "df_expanded_annotations = pd.DataFrame(expanded_annotations)\n",
    "\n",
    "# Procesar cada archivo de características\n",
    "for filename in os.listdir(features_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        df_features = pd.read_csv(os.path.join(features_directory, filename))\n",
    "        \n",
    "        # Fusionar los DataFrames basado en 'nombre_del_video' (ajustar si el nombre del archivo es el ID) y 'frame'\n",
    "        df_merged = pd.merge(df_features, df_expanded_annotations, on=['nombre_del_video', 'frame'], how='left')\n",
    "        \n",
    "        # Guardar el DataFrame combinado\n",
    "        df_merged.to_csv(os.path.join(output_directory, f\"{filename[:-4]}_merged.csv\"), index=False)\n",
    "\n",
    "print(\"Todos los archivos han sido procesados y fusionados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al directorio con los archivos fusionados\n",
    "merged_directory = 'D:/Codes/EFBA/DataEfba/mergeddata'\n",
    "\n",
    "# Directorios para guardar los datos escalados\n",
    "scaled_output_directory = 'D:/Codes/EFBA/DataEfba/scaleddata'\n",
    "if not os.path.exists(scaled_output_directory):\n",
    "    os.makedirs(scaled_output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mache12\\AppData\\Local\\Temp\\ipykernel_22820\\130129009.py:4: DtypeWarning: Columns (134,136) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesado y guardado: D:/Codes/EFBA/DataEfba/scaleddata\\allen_keypoints_processed_normalized_merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mache12\\AppData\\Local\\Temp\\ipykernel_22820\\130129009.py:4: DtypeWarning: Columns (134,136) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesado y guardado: D:/Codes/EFBA/DataEfba/scaleddata\\bird_keypoints_processed_normalized_merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mache12\\AppData\\Local\\Temp\\ipykernel_22820\\130129009.py:4: DtypeWarning: Columns (134,136) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesado y guardado: D:/Codes/EFBA/DataEfba/scaleddata\\curry1_keypoints_processed_normalized_merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mache12\\AppData\\Local\\Temp\\ipykernel_22820\\130129009.py:4: DtypeWarning: Columns (134,136) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesado y guardado: D:/Codes/EFBA/DataEfba/scaleddata\\kerr_keypoints_processed_normalized_merged.csv\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(merged_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(merged_directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Identificar columnas numéricas para escalar\n",
    "        cols_to_scale = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        cols_non_numeric = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "        \n",
    "        # Escalar solo las columnas numéricas\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled = pd.DataFrame(scaler.fit_transform(df[cols_to_scale]), columns=cols_to_scale, index=df.index)\n",
    "        \n",
    "        # Reintegra las columnas no numéricas\n",
    "        df_final_scaled = pd.concat([df_scaled, df[cols_non_numeric]], axis=1)\n",
    "        \n",
    "        # Guardar el DataFrame escalado\n",
    "        output_path = os.path.join(scaled_output_directory, filename)\n",
    "        df_final_scaled.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"Procesado y guardado: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Asumiendo que 'df_final_scaled' es tu DataFrame final que incluye tanto características como etiquetas\n",
    "# Define tus características y etiquetas\n",
    "X = df_final_scaled.drop(['criterio', 'correcto_o_incorrecto'], axis=1)  # Ajusta esto basado en tus nombres de columnas de etiquetas\n",
    "y = df_final_scaled[['criterio', 'correcto_o_incorrecto']]  # Ajusta esto según tus necesidades\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# Inicializar el clasificador RandomForest\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Hacer que el modelo soporte la clasificación multietiqueta\n",
    "model = MultiOutputClassifier(forest, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# Identificar columnas numéricas para imputar\n",
    "cols_to_impute = X_train.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Aplicar imputación solo a columnas numéricas\n",
    "imputer_X = SimpleImputer(strategy='mean')\n",
    "X_train[cols_to_impute] = imputer_X.fit_transform(X_train[cols_to_impute])\n",
    "X_test[cols_to_impute] = imputer_X.transform(X_test[cols_to_impute])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar columnas no numéricas\n",
    "cols_non_numeric = X_train.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# Aplicar imputación de la moda para columnas no numéricas\n",
    "imputer_non_num = SimpleImputer(strategy='most_frequent')\n",
    "X_train[cols_non_numeric] = imputer_non_num.fit_transform(X_train[cols_non_numeric])\n",
    "X_test[cols_non_numeric] = imputer_non_num.transform(X_test[cols_non_numeric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN en X_train después de la imputación: 0\n",
      "NaN en X_test después de la imputación: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"NaN en X_train después de la imputación:\", X_train.isnull().sum().sum())\n",
    "print(\"NaN en X_test después de la imputación:\", X_test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imputar las etiquetas, considerando que podrían ser categóricas\n",
    "imputer_y = SimpleImputer(strategy='most_frequent')\n",
    "y_train = pd.DataFrame(imputer_y.fit_transform(y_train), columns=y_train.columns)\n",
    "y_test = pd.DataFrame(imputer_y.transform(y_test), columns=y_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir características a numéricas si no lo son\n",
    "for col in X_train.columns:\n",
    "    X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
    "    X_test[col] = pd.to_numeric(X_test[col], errors='coerce')\n",
    "\n",
    "# Si las etiquetas deberían ser categóricas, asegúrate de que no se traten como números\n",
    "y_train = y_train.astype('category')\n",
    "y_test = y_test.astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mache12\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\impute\\_base.py:558: UserWarning: Skipping features without any observed values: ['nombre_del_video' 'jugador']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (8252, 133), indices imply (8252, 135)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m imputer_X \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimputer_X\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(imputer_X\u001b[38;5;241m.\u001b[39mtransform(X_test), columns\u001b[38;5;241m=\u001b[39mX_test\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:782\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    771\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    772\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    773\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    779\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[0;32m    780\u001b[0m         )\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 782\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    334\u001b[0m )\n\u001b[1;32m--> 336\u001b[0m \u001b[43m_check_values_indices_shape_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (8252, 133), indices imply (8252, 135)"
     ]
    }
   ],
   "source": [
    "imputer_X = SimpleImputer(strategy='mean')\n",
    "X_train = pd.DataFrame(imputer_X.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(imputer_X.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\multioutput.py:538\u001b[0m, in \u001b[0;36mMultiOutputClassifier.fit\u001b[1;34m(self, X, Y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[0;32m    513\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and targets Y.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \n\u001b[0;32m    515\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;124;03m        Returns a fitted instance.\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m [estimator\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;28;01mfor\u001b[39;00m estimator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_]\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\multioutput.py:245\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    242\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m=\u001b[39my, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 245\u001b[0m     \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my must have at least two dimensions for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti-output regression but has only one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    251\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:208\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_classification_targets\u001b[39m(y):\n\u001b[0;32m    197\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Ensure that target y is of a non-regression type.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m    Only the following target types (as defined in type_of_target) are allowed:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Target values.\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    215\u001b[0m     ]:\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    217\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Maybe you are trying to fit a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier, which expects discrete classes on a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression target with continuous values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:311\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse_pandas:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseSeries\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseArray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_multilabel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;66;03m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# We therefore catch both deprecation (NumPy < 1.24) warning and\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# value error (NumPy >= 1.24).\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\multiclass.py:189\u001b[0m, in \u001b[0;36mis_multilabel\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    184\u001b[0m         \u001b[38;5;28mlen\u001b[39m(y\u001b[38;5;241m.\u001b[39mdata) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (labels\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (labels\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01min\u001b[39;00m labels))\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (y\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbiu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m _is_integral_float(labels))  \u001b[38;5;66;03m# bool, int, uint\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     )\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    192\u001b[0m         y\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbiu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m _is_integral_float(labels)  \u001b[38;5;66;03m# bool, int, uint\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_array_api.py:262\u001b[0m, in \u001b[0;36m_NumPyAPIWrapper.unique_values\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 336\u001b[0m     \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[0;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predecir con el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular la precisión\n",
    "print(\"Precisión:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir el espacio de parámetros para Grid Search\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [50, 100, 200],\n",
    "    'estimator__max_depth': [None, 10, 20, 30]\n",
    "}\n",
    "\n",
    "# Configurar Grid Search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores parámetros:\", grid_search.best_params_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
